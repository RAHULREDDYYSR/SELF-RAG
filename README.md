# SELF\_RAG: Self-Reflective Retrieval-Augmented Generation

**SELF\_RAG** is a self-evaluating Retrieval-Augmented Generation (RAG) system inspired by the paper [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/pdf/2310.11511). It extends the standard RAG pipeline with multiple grading stages that assess the relevance, accuracy, and hallucination level of generated answers, allowing the system to refine and improve its own output.

---

## ðŸ“ Project Structure

```bash
SELF_RAG/
â”‚
â”œâ”€â”€ .chroma/                     # ChromaDB vector store
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ answer_grader.py         # Grades the generated answer against the original question
â”‚   â”‚   â”‚   â”œâ”€â”€ generation.py            # Generation step
â”‚   â”‚   â”‚   â”œâ”€â”€ hallucination_grader.py # Grades grounding of generation in retrieved docs
â”‚   â”‚   â”‚   â””â”€â”€ retrieval_grader.py     # Grades doc relevance to input question
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ consts.py                    # Constants used across the graph
â”‚   â”‚   â”œâ”€â”€ graph.py                     # LangGraph graph definition
â”‚   â”‚   â””â”€â”€ state.py                     # Shared state definition for LangGraph
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ingestion.py                 # Script to ingest and embed documents into ChromaDB
â”œâ”€â”€ main.py                      # Entry point to run the SELF_RAG pipeline
â”œâ”€â”€ graph.png                    # Architecture diagram of the graph flow
â”œâ”€â”€ .env                         # Environment variables (e.g., API keys)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Pipfile                      # Dependency management
â”œâ”€â”€ Pipfile.lock
â””â”€â”€ README.md                    # You're here!
```

---

## ðŸ§  How It Works

SELF\_RAG follows a multi-stage decision-making process using LangGraph. The pipeline includes:

1. **Document Retrieval**

   * Uses embedded document chunks stored in ChromaDB.
   * Filters top-k relevant documents.

2. **Relevance Grading**

   * Each document is evaluated for relevance to the userâ€™s query using a relevance grader.

3. **Answer Generation**

   * A response is generated based on the top-ranked relevant documents.

4. **Hallucination Grading**

   * Assesses if the generated answer is well-grounded in the retrieved documents.

5. **Answer Grading**

   * Evaluates the generated answerâ€™s quality and correctness in relation to the original question.

6. **Decision Making**

   * If the answer lacks grounding or is inaccurate, the system attempts regeneration.

---

## âœ… Example Output (Terminal)

```bash
$ python main.py
Data_Directory doesn't exist, creating one...
Done!
Self_RAG in work...
ðŸ“„ Retrieving documents...
ðŸ” CHECK DOCUMENT RELEVANCE TO QUESTION...
âœ… GRADE: DOCUMENT(1) RELEVANT
âœ… GRADE: DOCUMENT(2) RELEVANT
âœ… GRADE: DOCUMENT(3) RELEVANT
âœ… GRADE: DOCUMENT(4) RELEVANT
âœ… FINISHED CHECKING DOCUMENT RELEVANCE TO QUESTION...
ðŸ§  ASSESS GRADED DOCUMENTS...
ðŸŸ¢ DECISION: GENERATE...
âš™ï¸  Generating...
ðŸ¤– CHECK HALLUCINATION...
ðŸŸ¢ DECISION: GENERATION IS GROUNDED IN DOCUMENTS
âœ… GRADE GENERATION VS QUESTION...
âœ… GRADE: GENERATION IS ANSWER TO QUESTION

{
  "question": "what is agent memory",
  "generation": "Agent memory refers to the mechanisms by which a large language model (LLM)-powered agent retains and recalls information...",
  "web_search": false,
  "documents": [...],
  "source": "https://lilianweng.github.io/posts/2023-06-23-agent/",
  "title": "LLM Powered Autonomous Agents | Lilâ€™Log"
}
```

---

## ðŸ”§ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/SELF_RAG.git
cd SELF_RAG
```

### 2. Install Dependencies

```bash
pip install pipenv
pipenv install
```

### 3. Set Up Environment Variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_openai_key_here
```

### 4. Ingest Your Documents

Put your custom `.txt`, `.pdf`, or `.md` files in a `/docs` folder.

Then run:

```bash
pipenv run python ingestion.py
```

### 5. Run the Pipeline

```bash
pipenv run python main.py
```

---

## ðŸ“Œ Inspiration

This project is directly inspired by the paper:

* [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/pdf/2310.11511)

SELF\_RAG brings this idea to Retrieval-Augmented Generation (RAG) by using grading steps between generations to reduce hallucinations and ensure relevance and answer quality.

It also leverages:

* [LangChain](https://github.com/langchain-ai/langchain)
* [LangGraph](https://github.com/langchain-ai/langgraph)
* [ChromaDB](https://github.com/chroma-core/chroma)
* [OpenAI APIs](https://platform.openai.com/)

---

## ðŸ“Š Diagram

Below is a visual representation of the LangGraph pipeline used in SELF\_RAG:

![SELF\_RAG Graph Architecture](graph.png)

> The system moves from document retrieval âžž relevance grading âžž answer generation âžž hallucination checking âžž final answer decision.

---

## ðŸš€ Future Work

Planned improvements for SELF\_RAG:

* [ ] Add multi-turn conversation memory
* [ ] Integrate web search for document retrieval
* [ ] Build a web-based UI for monitoring the pipeline
* [ ] Use other grading models (Claude, Gemini, Mixtral)
* [ ] Support long context inputs (e.g. via Claude 3 or GPT-4-turbo)

---

## ðŸ“„ License

This project is licensed under the MIT License.

---

## ðŸ™Œ Acknowledgments

* [LangChain](https://github.com/langchain-ai/langchain)
* [LangGraph](https://github.com/langchain-ai/langgraph)
* [ChromaDB](https://github.com/chroma-core/chroma)
* [OpenAI](https://openai.com/)
